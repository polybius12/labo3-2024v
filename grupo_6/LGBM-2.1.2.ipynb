{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#########                 Modelo LightGBM                 ###########\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device name: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Tensor on CUDA: tensor([0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Carga de Librerias\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.stats import yeojohnson\n",
    "\n",
    "import torch                                     \n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()      \n",
    "print(f\"CUDA available: {cuda_available}\")      \n",
    "\n",
    "# Check the device name                         \n",
    "if cuda_available:                              \n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"CUDA device name: {device_name}\")\n",
    "\n",
    "    # Move a tensor to the GPU and print it\n",
    "    tensor = torch.zeros(1).cuda()\n",
    "    print(f\"Tensor on CUDA: {tensor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de datos\n",
    "\n",
    "#Ventas por customer y periodo\n",
    "sell_in = pd.read_csv(\"Datos/sell-in.txt\", delimiter=\"\\t\")  \n",
    "\n",
    "#Informacion de los productos\n",
    "productos_descripcion = pd.read_csv(\"Datos/tb_productos_descripcion.txt\", delimiter=\"\\t\")  \n",
    "\n",
    "#Productos a predecir\n",
    "productos_a_predecir = pd.read_csv(\"Datos/productos_a_predecir.txt\", delimiter=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de product_id diferentes es: 780\n",
      "\n",
      "\n",
      "Datos de los 780 productos\n",
      "\n",
      "| periodo   | customer_id   | product_id   | plan_precios_cuidados   | cust_request_qty   | cust_request_tn   | tn      |\n",
      "|:----------|:--------------|:-------------|:------------------------|:-------------------|:------------------|:--------|\n",
      "| 201701    | 10234         | 20524        | 0                       | 2                  | 0.053             | 0.053   |\n",
      "| 201701    | 10032         | 20524        | 0                       | 1                  | 0.13628           | 0.13628 |\n",
      "| 201701    | 10217         | 20524        | 0                       | 1                  | 0.03028           | 0.03028 |\n",
      "| 201701    | 10125         | 20524        | 0                       | 1                  | 0.02271           | 0.02271 |\n",
      "| 201701    | 10012         | 20524        | 0                       | 11                 | 1.54452           | 1.54452 |\n",
      "\n",
      "\n",
      "Datos Info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2293481 entries, 0 to 2945817\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   periodo                int64  \n",
      " 1   customer_id            int64  \n",
      " 2   product_id             int64  \n",
      " 3   plan_precios_cuidados  int64  \n",
      " 4   cust_request_qty       int64  \n",
      " 5   cust_request_tn        float64\n",
      " 6   tn                     float64\n",
      "dtypes: float64(2), int64(5)\n",
      "memory usage: 140.0 MB\n",
      "None\n",
      "\n",
      "\n",
      "Hay 0 filas duplicadas en el DataFrame.\n"
     ]
    }
   ],
   "source": [
    "#Primer filtro y Visualizacion\n",
    "\n",
    "#Filtrar por los 780 productos\n",
    "filtered_sell_in = sell_in[sell_in['product_id'].isin(productos_a_predecir['product_id'])]\n",
    "\n",
    "# Contar el número de product_id únicos\n",
    "num_productos_diferentes = filtered_sell_in['product_id'].nunique()\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(f\"El número de product_id diferentes es: {num_productos_diferentes}\")\n",
    "\n",
    "print(\"\\n\\nDatos de los 780 productos\\n\")\n",
    "print(filtered_sell_in.head().to_markdown(index=False, numalign='left', stralign='left'))\n",
    "\n",
    "print(\"\\n\\nDatos Info:\\n\")\n",
    "print(filtered_sell_in.info())\n",
    "\n",
    "# Verificar si hay filas duplicadas en el DataFrame\n",
    "duplicados = filtered_sell_in.duplicated()\n",
    "\n",
    "# Contar el número de filas duplicadas\n",
    "num_duplicados = duplicados.sum()\n",
    "\n",
    "# Imprimir el número de filas duplicadas\n",
    "print(f'\\n\\nHay {num_duplicados} filas duplicadas en el DataFrame.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id           tn\n",
      "0       20001  50340.39558\n",
      "1       20002  36337.25439\n",
      "2       20003  32004.15274\n",
      "3       20004  24178.15379\n",
      "4       20005  23191.21852\n"
     ]
    }
   ],
   "source": [
    "#Top 5 productos de los 780 a predecir\n",
    "\n",
    "# Agrupar por 'product_id', sumar 'tn' y ordenar\n",
    "product_totals = filtered_sell_in.groupby('product_id')['tn'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Seleccionar los 5 primeros productos\n",
    "top_5_products = product_totals.head(5).reset_index()\n",
    "\n",
    "print(top_5_products)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir funcion para elegir que dataset usar: \"780 productos\" o \"top 5 productos\"\n",
    "\n",
    "def productos_780(filtered_sell_in, top_5_products):\n",
    "    # Código para cuando el valor es 1\n",
    "    \n",
    "    # Contar el número de product_id únicos\n",
    "    num_productos_diferentes = filtered_sell_in['product_id'].nunique()\n",
    "    \n",
    "    # Imprimir el resultado\n",
    "    print(f\"El número de product_id diferentes es: {num_productos_diferentes}\")\n",
    "\n",
    "    print(\"\\n\\nDatos de los 780 productos\\n\")\n",
    "    print(filtered_sell_in.head().to_markdown(index=False, numalign='left', stralign='left'))\n",
    "\n",
    "    print(\"\\n\\nDatos Info:\\n\")\n",
    "    print(filtered_sell_in.info())\n",
    "\n",
    "    # Contar el número de product_id únicos\n",
    "    num_productos_diferentes = filtered_sell_in['product_id'].nunique()\n",
    "\n",
    "    # Verificar si hay filas duplicadas en el DataFrame\n",
    "    duplicados = filtered_sell_in.duplicated()\n",
    "\n",
    "    # Contar el número de filas duplicadas\n",
    "    num_duplicados = duplicados.sum()\n",
    "\n",
    "    # Imprimir el número de filas duplicadas\n",
    "    print(f'\\n\\nHay {num_duplicados} filas duplicadas en el DataFrame.')\n",
    "    \n",
    "    data = filtered_sell_in\n",
    "    print(\"\\n\\nFunción para 780 prod ejecutada.\")\n",
    "    return filtered_sell_in # Devuelve el DataFrame filtrado\n",
    "\n",
    "    \n",
    "\n",
    "def productos_top5(filtered_sell_in, top_5_products):\n",
    "    # Código para cuando el valor es 0\n",
    "        \n",
    "    filtered_sell_in_top5 = filtered_sell_in[filtered_sell_in['product_id'].isin(top_5_products['product_id'])].reset_index(drop=True)\n",
    "\n",
    "    # Contar el número de product_id únicos\n",
    "    num_productos_diferentes = filtered_sell_in_top5['product_id'].nunique()\n",
    "\n",
    "    # Imprimir el resultado\n",
    "    print(f\"El número de product_id diferentes es: {num_productos_diferentes}\")\n",
    "\n",
    "    print(\"\\n\\nDatos de los 5 productos\\n\")\n",
    "    print(filtered_sell_in_top5.head().to_markdown(index=False, numalign='left', stralign='left'))\n",
    "\n",
    "    print(\"\\n\\nDatos Info:\\n\")\n",
    "    print(filtered_sell_in_top5.info())\n",
    "\n",
    "    # Contar el número de product_id únicos\n",
    "    num_productos_diferentes = filtered_sell_in_top5['product_id'].nunique()\n",
    "\n",
    "    # Verificar si hay filas duplicadas en el DataFrame\n",
    "    duplicados = filtered_sell_in_top5.duplicated()\n",
    "\n",
    "    # Contar el número de filas duplicadas\n",
    "    num_duplicados = duplicados.sum()\n",
    "\n",
    "    # Imprimir el número de filas duplicadas\n",
    "    print(f'\\n\\nHay {num_duplicados} filas duplicadas en el DataFrame.')\n",
    "    \n",
    "    data = filtered_sell_in_top5\n",
    "    print(\"\\n\\nFunción para top 5 prod ejecutada.\")\n",
    "    return filtered_sell_in_top5  # Devuelve el DataFrame filtrado\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Diccionario que actúa como switcher\n",
    "switcher = {\n",
    "    1: productos_780,\n",
    "    0: productos_top5,\n",
    "}\n",
    "\n",
    "# Función que usa el switcher (modificada)\n",
    "def ejecutar_funcion(valor, filtered_sell_in, top_5_products):  \n",
    "    func = switcher.get(valor, lambda: \"Valor inválido\")\n",
    "    return func(filtered_sell_in, top_5_products)  # Pasa los DataFrames a la función\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de product_id diferentes es: 780\n",
      "\n",
      "\n",
      "Datos de los 780 productos\n",
      "\n",
      "| periodo   | customer_id   | product_id   | plan_precios_cuidados   | cust_request_qty   | cust_request_tn   | tn      |\n",
      "|:----------|:--------------|:-------------|:------------------------|:-------------------|:------------------|:--------|\n",
      "| 201701    | 10234         | 20524        | 0                       | 2                  | 0.053             | 0.053   |\n",
      "| 201701    | 10032         | 20524        | 0                       | 1                  | 0.13628           | 0.13628 |\n",
      "| 201701    | 10217         | 20524        | 0                       | 1                  | 0.03028           | 0.03028 |\n",
      "| 201701    | 10125         | 20524        | 0                       | 1                  | 0.02271           | 0.02271 |\n",
      "| 201701    | 10012         | 20524        | 0                       | 11                 | 1.54452           | 1.54452 |\n",
      "\n",
      "\n",
      "Datos Info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2293481 entries, 0 to 2945817\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   periodo                int64  \n",
      " 1   customer_id            int64  \n",
      " 2   product_id             int64  \n",
      " 3   plan_precios_cuidados  int64  \n",
      " 4   cust_request_qty       int64  \n",
      " 5   cust_request_tn        float64\n",
      " 6   tn                     float64\n",
      "dtypes: float64(2), int64(5)\n",
      "memory usage: 140.0 MB\n",
      "None\n",
      "\n",
      "\n",
      "Hay 0 filas duplicadas en el DataFrame.\n",
      "\n",
      "\n",
      "Función para 780 prod ejecutada.\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################################\n",
    "#############################                 Elegir el dataframe a utilizar                 #############################\n",
    "##########################################################################################################################\n",
    "\n",
    "# 1: productos_780,\n",
    "# 0: productos_top5\n",
    "\n",
    "\n",
    "data = ejecutar_funcion(1, \n",
    "                        filtered_sell_in, \n",
    "                        top_5_products)  # Guarda el resultado en 'data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ventas totales por periodo y producto:\n",
      "\n",
      "| periodo   | customer_id   | product_id   | total_tn   |\n",
      "|:----------|:--------------|:-------------|:-----------|\n",
      "| 201701    | 10001         | 20001        | 99.4386    |\n",
      "| 201701    | 10001         | 20002        | 87.6486    |\n",
      "| 201701    | 10001         | 20003        | 100.213    |\n",
      "| 201701    | 10001         | 20004        | 21.7395    |\n",
      "| 201701    | 10001         | 20006        | 29.172     |\n",
      "\n",
      "\n",
      "Datos Info productos agrupados por mes:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2293481 entries, 0 to 2293480\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   periodo      int64  \n",
      " 1   customer_id  int64  \n",
      " 2   product_id   int64  \n",
      " 3   total_tn     float64\n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 70.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Agrupacion de datos mensuales\n",
    "\n",
    "# Agrupar por periodo y product_id, sumar 'tn' y resetear el índice\n",
    "sales_by_period_product = (data.groupby(['periodo', 'customer_id','product_id'])['tn'].sum()\n",
    "                                          .reset_index()\n",
    "                                          .rename(columns={'tn': 'total_tn'}))\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"\\nVentas totales por periodo y producto:\\n\")\n",
    "print(sales_by_period_product.head().to_markdown(index=False, numalign='left', stralign='left'))\n",
    "\n",
    "print(\"\\n\\nDatos Info productos agrupados por mes:\\n\")\n",
    "print(sales_by_period_product.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| product_id   | customer_id   | periodo   | total_tn   |\n",
      "|:-------------|:--------------|:----------|:-----------|\n",
      "| 20001        | 10001         | 201701    | 99.4386    |\n",
      "| 20001        | 10001         | 201702    | 198.844    |\n",
      "| 20001        | 10001         | 201703    | 92.4654    |\n",
      "| 20001        | 10001         | 201704    | 13.2973    |\n",
      "| 20001        | 10001         | 201705    | 101.006    |\n",
      "\n",
      "\n",
      "Datos Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16763760 entries, 0 to 16763759\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   product_id   int64  \n",
      " 1   customer_id  int64  \n",
      " 2   periodo      int64  \n",
      " 3   total_tn     float64\n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 511.6 MB\n",
      "None\n",
      "\n",
      "\n",
      "Todos los periodos estan completos.\n"
     ]
    }
   ],
   "source": [
    "#Completar y Equilibrar datos de Ventas por Producto, Cliente y Periodo\n",
    "\n",
    "#Obtener todos los productos y periodos únicos\n",
    "all_product_ids = sales_by_period_product['product_id'].unique()\n",
    "all_customers = sales_by_period_product['customer_id'].unique()\n",
    "all_periods = sales_by_period_product['periodo'].unique()\n",
    "\n",
    " \n",
    "# Crear un DataFrame con todas las combinaciones posibles de product_id y periodo\n",
    "all_combinations = pd.MultiIndex.from_product([all_product_ids,all_customers, all_periods], names=['product_id','customer_id', 'periodo']).to_frame(index=False)\n",
    " \n",
    "# Realizar el merge con el DataFrame original, llenando los valores faltantes con 0\n",
    "df_complete = pd.merge(all_combinations, sales_by_period_product, on=['product_id','customer_id', 'periodo'], how='left').fillna(0)\n",
    " \n",
    "# Verificar el resultado\n",
    "print(df_complete.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "print(\"\\n\\nDatos Info\")\n",
    "print(df_complete.info())\n",
    "\n",
    "# Control de que esté completo\n",
    "# Contar filas por combinación de customer_id y product_id\n",
    "rows_per_combination = df_complete.groupby(['customer_id', 'product_id']).size()\n",
    "\n",
    "# Verificar si todos los valores son 36\n",
    "if all(rows_per_combination == 36):\n",
    "    print(\"\\n\\nTodos los periodos estan completos.\")\n",
    "else:\n",
    "    print(\"\\n\\nNo todos los estan completos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Cantidad de combinaciones cliente-producto que nunca tuvieron venta: 202855\n",
      "2. Cantidad de veces que un cliente dejó de comprar un producto en los últimos 6 periodos: 6189\n",
      "3. Cantidad de veces que un producto empezó a comprarse en los últimos 6 meses: 21857\n"
     ]
    }
   ],
   "source": [
    "# Analisis de compras por cliente\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# 1. Cantidad de veces que un cliente-producto nunca en la historia tuvo venta\n",
    "nunca_vendidos = df_complete.groupby(['product_id', 'customer_id'])['total_tn'].sum().eq(0).sum()\n",
    "\n",
    "print(f\"1. Cantidad de combinaciones cliente-producto que nunca tuvieron venta: {nunca_vendidos}\")\n",
    "\n",
    "# 2. Cantidad de veces que un cliente dejó de comprar un producto en los últimos 6 periodos\n",
    "df_complete['periodo'] = pd.to_datetime(df_complete['periodo'], format='%Y%m')\n",
    "ultimo_periodo = df_complete['periodo'].max()\n",
    "ultimos_6_periodos = [ultimo_periodo - pd.DateOffset(months=i) for i in range(6)]\n",
    "\n",
    "def cliente_dejo_comprar(group):\n",
    "    ultimas_compras = group[group['periodo'].isin(ultimos_6_periodos)]['total_tn']\n",
    "    return (ultimas_compras.iloc[0] > 0) & (ultimas_compras.iloc[1:] == 0).all()\n",
    "\n",
    "dejo_comprar = df_complete.sort_values('periodo', ascending=False).groupby(['product_id', 'customer_id'], group_keys=False).apply(cliente_dejo_comprar).sum()\n",
    "\n",
    "print(f\"2. Cantidad de veces que un cliente dejó de comprar un producto en los últimos 6 periodos: {dejo_comprar}\")\n",
    "\n",
    "# 3. Cantidad de veces que un producto empezó a comprarse en los últimos 6 meses\n",
    "def producto_empezo_comprarse(group):\n",
    "    compras_historicas = group[group['periodo'] < ultimos_6_periodos[-1]]['total_tn']\n",
    "    compras_recientes = group[group['periodo'].isin(ultimos_6_periodos)]['total_tn']\n",
    "    return (compras_historicas == 0).all() & (compras_recientes > 0).any()\n",
    "\n",
    "empezo_comprarse = df_complete.sort_values('periodo').groupby(['product_id', 'customer_id'], group_keys=False).apply(producto_empezo_comprarse).sum()\n",
    "\n",
    "print(f\"3. Cantidad de veces que un producto empezó a comprarse en los últimos 6 meses: {empezo_comprarse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño original del DataFrame: 16763760\n",
      "Tamaño del DataFrame después de eliminar combinaciones sin ventas: 9460980\n",
      "Tamaño del DataFrame después de eliminar combinaciones inactivas: 9238176\n",
      "DataFrame guardado como 'df_activos.csv'\n"
     ]
    }
   ],
   "source": [
    "# Asumimos que tu DataFrame se llama 'df_complete'\n",
    "\n",
    "# 1. Eliminar combinaciones cliente-producto que nunca se vendieron\n",
    "df_vendidos = df_complete.groupby(['product_id', 'customer_id']).filter(lambda x: (x['total_tn'] > 0).any())\n",
    "\n",
    "print(f\"Tamaño original del DataFrame: {len(df_complete)}\")\n",
    "print(f\"Tamaño del DataFrame después de eliminar combinaciones sin ventas: {len(df_vendidos)}\")\n",
    "\n",
    "# 2. Identificar y eliminar combinaciones cliente-producto que ya no van a comprar\n",
    "df_vendidos['periodo'] = pd.to_datetime(df_vendidos['periodo'], format='%Y%m')\n",
    "ultimo_periodo = df_vendidos['periodo'].max()\n",
    "ultimos_6_periodos = [ultimo_periodo - pd.DateOffset(months=i) for i in range(6)]\n",
    "\n",
    "def cliente_sigue_comprando(group):\n",
    "    ultimas_compras = group[group['periodo'].isin(ultimos_6_periodos)]['total_tn']\n",
    "    return not ((ultimas_compras.iloc[0] > 0) & (ultimas_compras.iloc[1:] == 0).all())\n",
    "\n",
    "df_activos = df_vendidos.sort_values('periodo', ascending=False).groupby(['product_id', 'customer_id']).filter(cliente_sigue_comprando)\n",
    "\n",
    "print(f\"Tamaño del DataFrame después de eliminar combinaciones inactivas: {len(df_activos)}\")\n",
    "\n",
    "# Guardar el DataFrame resultante\n",
    "df_activos.to_csv('df_activos.csv', index=False)\n",
    "print(\"DataFrame guardado como 'df_activos.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Datos de los registros activos\n",
      "\n",
      "| product_id   | customer_id   | periodo    | total_tn   |\n",
      "|:-------------|:--------------|:-----------|:-----------|\n",
      "| 21214        | 10550         | 2019-12-01 | 0          |\n",
      "| 20130        | 10042         | 2019-12-01 | 1.68015    |\n",
      "| 20130        | 10061         | 2019-12-01 | 0.84008    |\n",
      "| 20140        | 10174         | 2019-12-01 | 0.0819     |\n",
      "| 20614        | 10506         | 2019-12-01 | 0          |\n",
      "\n",
      "\n",
      "Datos Info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9238176 entries, 0 to 9238175\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   product_id   int64  \n",
      " 1   customer_id  int64  \n",
      " 2   periodo      object \n",
      " 3   total_tn     float64\n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 281.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Ventas por customer y periodo\n",
    "df_activos2 = pd.read_csv(\"df_activos.csv\")  \n",
    "\n",
    "print(\"\\n\\nDatos de los registros activos\\n\")\n",
    "print(df_activos2.head().to_markdown(index=False, numalign='left', stralign='left'))\n",
    "\n",
    "print(\"\\n\\nDatos Info:\\n\")\n",
    "print(df_activos2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ventas totales por periodo y producto con descripcion:\n",
      "| product_id   | customer_id   | periodo    | total_tn   | cat1   | cat2           | cat3          | brand    | sku_size   | descripcion   |\n",
      "|:-------------|:--------------|:-----------|:-----------|:-------|:---------------|:--------------|:---------|:-----------|:--------------|\n",
      "| 21214        | 10550         | 2019-12-01 | 0          | PC     | DEOS           | RollOn        | NIVEA    | 50         | Aroma 14      |\n",
      "| 20130        | 10042         | 2019-12-01 | 1.68015    | PC     | CABELLO        | SHAMPOO       | SHAMPOO2 | 930        | Manzana       |\n",
      "| 20130        | 10061         | 2019-12-01 | 0.84008    | PC     | CABELLO        | SHAMPOO       | SHAMPOO2 | 930        | Manzana       |\n",
      "| 20140        | 10174         | 2019-12-01 | 0.0819     | PC     | PIEL2          | Jabon Regular | DEOS1    | 125        | Orquideas     |\n",
      "| 20614        | 10506         | 2019-12-01 | 0          | FOODS  | SOPAS Y CALDOS | Baking Bags   | MAGGI    | 15         | Sabor 6       |\n",
      "\n",
      "\n",
      "Datos Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9238176 entries, 0 to 9238175\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   product_id   int64  \n",
      " 1   customer_id  int64  \n",
      " 2   periodo      object \n",
      " 3   total_tn     float64\n",
      " 4   cat1         object \n",
      " 5   cat2         object \n",
      " 6   cat3         object \n",
      " 7   brand        object \n",
      " 8   sku_size     int64  \n",
      " 9   descripcion  object \n",
      "dtypes: float64(1), int64(3), object(6)\n",
      "memory usage: 704.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Merge de datos de productos\n",
    "\n",
    "# Realizar el merge entre `sales_by_period_product` y `productos_descripcion`\n",
    "merged_df = pd.merge(df_activos2, productos_descripcion, on='product_id', how='left')\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "print(\"\\nVentas totales por periodo y producto con descripcion:\")\n",
    "print(merged_df.head().to_markdown(index=False, numalign='left', stralign='left'))\n",
    "\n",
    "print(\"\\n\\nDatos Info\")\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hay un total de 0 valores nulos en el DataFrame.\n",
      "\n",
      "Valores nulos por columna:\n",
      "\n",
      "product_id     0\n",
      "customer_id    0\n",
      "periodo        0\n",
      "total_tn       0\n",
      "cat1           0\n",
      "cat2           0\n",
      "cat3           0\n",
      "brand          0\n",
      "sku_size       0\n",
      "descripcion    0\n",
      "dtype: int64\n",
      "\n",
      "Hay un total de 6982654 valores iguales a cero en el DataFrame.\n",
      "\n",
      "Valores iguales a cero por columna:\n",
      "product_id           0\n",
      "customer_id          0\n",
      "periodo              0\n",
      "total_tn       6982654\n",
      "cat1                 0\n",
      "cat2                 0\n",
      "cat3                 0\n",
      "brand                0\n",
      "sku_size             0\n",
      "descripcion          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Control de Nulos y Ceros\n",
    "\n",
    "# Contar valores nulos en todo el DataFrame\n",
    "total_nulos = merged_df.isnull().sum().sum()\n",
    "print(f'\\nHay un total de {total_nulos} valores nulos en el DataFrame.')\n",
    "\n",
    "# Contar valores nulos por columna\n",
    "nulos_por_columna = merged_df.isnull().sum()\n",
    "print('\\nValores nulos por columna:\\n')\n",
    "print(nulos_por_columna)\n",
    "\n",
    "# Contar valores iguales a cero en todo el DataFrame\n",
    "total_ceros = (merged_df == 0).sum().sum()\n",
    "print(f'\\nHay un total de {total_ceros} valores iguales a cero en el DataFrame.')\n",
    "\n",
    "# Contar valores iguales a cero por columna\n",
    "ceros_por_columna = (merged_df == 0).sum()\n",
    "print('\\nValores iguales a cero por columna:')\n",
    "print(ceros_por_columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaracion de funciones para el tipo de escalado\n",
    "\n",
    "def scale_data(df, method='standard'):\n",
    "    # Crear una copia del DataFrame\n",
    "    scaled_df = df.copy()\n",
    "    \n",
    "    # Agrupar por product_id y customer_id\n",
    "    grouped = scaled_df.groupby(['product_id', 'customer_id'])\n",
    "    \n",
    "    # Función para aplicar el escalado a cada grupo\n",
    "    def scale_group(group):\n",
    "        if method == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "            group['total_tn_scaled'] = scaler.fit_transform(group[['total_tn']])\n",
    "        elif method == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "            group['total_tn_scaled'] = scaler.fit_transform(group[['total_tn']])\n",
    "        elif method == 'log':\n",
    "            group['total_tn_scaled'] = np.log1p(group['total_tn'])\n",
    "        elif method == 'yeo-johnson':\n",
    "            group['total_tn_scaled'], _ = yeojohnson(group['total_tn'])\n",
    "        elif method == 'quantile':\n",
    "            scaler = QuantileTransformer(output_distribution='normal')\n",
    "            group['total_tn_scaled'] = scaler.fit_transform(group[['total_tn']])\n",
    "        else:\n",
    "            raise ValueError(\"Método de escalado no reconocido\")\n",
    "        \n",
    "        return group\n",
    "\n",
    "    # Aplicar el escalado a cada grupo\n",
    "    scaled_df = grouped.apply(scale_group).reset_index(drop=True)\n",
    "    \n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ventas escaladas por periodo y producto con descripcion:\n",
      "| product_id   | customer_id   | periodo    | total_tn   | cat1   | cat2        | cat3    | brand   | sku_size   | descripcion   | total_tn_scaled   |\n",
      "|:-------------|:--------------|:-----------|:-----------|:-------|:------------|:--------|:--------|:-----------|:--------------|:------------------|\n",
      "| 20001        | 10001         | 2019-12-01 | 180.219    | HC     | ROPA LAVADO | Liquido | ARIEL   | 3000       | genoma        | 5.19971           |\n",
      "| 20001        | 10001         | 2019-11-01 | 236.656    | HC     | ROPA LAVADO | Liquido | ARIEL   | 3000       | genoma        | 5.47082           |\n",
      "| 20001        | 10001         | 2019-10-01 | 176.03     | HC     | ROPA LAVADO | Liquido | ARIEL   | 3000       | genoma        | 5.17632           |\n",
      "| 20001        | 10001         | 2019-09-01 | 109.052    | HC     | ROPA LAVADO | Liquido | ARIEL   | 3000       | genoma        | 4.70096           |\n",
      "| 20001        | 10001         | 2019-08-01 | 33.6399    | HC     | ROPA LAVADO | Liquido | ARIEL   | 3000       | genoma        | 3.54501           |\n",
      "\n",
      "\n",
      "Datos Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9238176 entries, 0 to 9238175\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   product_id       int64  \n",
      " 1   customer_id      int64  \n",
      " 2   periodo          object \n",
      " 3   total_tn         float64\n",
      " 4   cat1             object \n",
      " 5   cat2             object \n",
      " 6   cat3             object \n",
      " 7   brand            object \n",
      " 8   sku_size         int64  \n",
      " 9   descripcion      object \n",
      " 10  total_tn_scaled  float64\n",
      "dtypes: float64(2), int64(3), object(6)\n",
      "memory usage: 775.3+ MB\n",
      "None\n",
      "\n",
      "\n",
      "Metodo de escalado:\n",
      "log\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################################################################################\n",
    "############################################                 Elegir el escalado a utilizar                 ############################################\n",
    "#######################################################################################################################################################\n",
    "\n",
    "#Los métodos disponibles en este switcher son:\n",
    "\n",
    "#  standard (StandardScaler)\n",
    "#  robust (RobustScaler)\n",
    "#  log (Transformación logarítmica)\n",
    "#  yeo-johnson (PowerTransformer con método Yeo-Johnson)\n",
    "#  quantile (QuantileTransformer con distribución normal)\n",
    "\n",
    "# Crear una copia del DataFrame\n",
    "data_scaled = merged_df.copy()\n",
    "\n",
    "# Ejemplo de uso\n",
    "method = 'log'  # Cambia este valor a 'standard', 'robust', 'log', 'yeo-johnson', o 'quantile' para probar diferentes métodos\n",
    "data_scaled = scale_data(data_scaled, method)\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "print(\"\\nVentas escaladas por periodo y producto con descripcion:\")\n",
    "print(data_scaled.head().to_markdown(index=False, numalign='left', stralign='left'))\n",
    "\n",
    "print(\"\\n\\nDatos Info\")\n",
    "print(data_scaled.info())\n",
    "\n",
    "print(\"\\n\\nMetodo de escalado:\")\n",
    "print(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hay un total de 0 valores nulos en el DataFrame.\n",
      "\n",
      "Valores nulos por columna:\n",
      "\n",
      "product_id         0\n",
      "customer_id        0\n",
      "periodo            0\n",
      "total_tn           0\n",
      "cat1               0\n",
      "cat2               0\n",
      "cat3               0\n",
      "brand              0\n",
      "sku_size           0\n",
      "descripcion        0\n",
      "total_tn_scaled    0\n",
      "dtype: int64\n",
      "\n",
      "Hay un total de 13965308 valores iguales a cero en el DataFrame.\n",
      "\n",
      "Valores iguales a cero por columna:\n",
      "product_id               0\n",
      "customer_id              0\n",
      "periodo                  0\n",
      "total_tn           6982654\n",
      "cat1                     0\n",
      "cat2                     0\n",
      "cat3                     0\n",
      "brand                    0\n",
      "sku_size                 0\n",
      "descripcion              0\n",
      "total_tn_scaled    6982654\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Control de Nulos y Ceros\n",
    "\n",
    "# Contar valores nulos en todo el DataFrame\n",
    "total_nulos = data_scaled.isnull().sum().sum()\n",
    "print(f'\\nHay un total de {total_nulos} valores nulos en el DataFrame.')\n",
    "\n",
    "# Contar valores nulos por columna\n",
    "nulos_por_columna = data_scaled.isnull().sum()\n",
    "print('\\nValores nulos por columna:\\n')\n",
    "print(nulos_por_columna)\n",
    "\n",
    "# Contar valores iguales a cero en todo el DataFrame\n",
    "total_ceros = (data_scaled == 0).sum().sum()\n",
    "print(f'\\nHay un total de {total_ceros} valores iguales a cero en el DataFrame.')\n",
    "\n",
    "# Contar valores iguales a cero por columna\n",
    "ceros_por_columna = (data_scaled == 0).sum()\n",
    "print('\\nValores iguales a cero por columna:')\n",
    "print(ceros_por_columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Columnas actuales\n",
      "\n",
      "| product_id   | customer_id   | parent_product   | sku_size   | sku_size_rank   | is_smallest_sku   | is_largest_sku   | market_share_in_parent   | avg_market_share_in_parent   |\n",
      "|:-------------|:--------------|:-----------------|:-----------|:----------------|:------------------|:-----------------|:-------------------------|:-----------------------------|\n",
      "| 20001        | 10001         | 56               | 3000       | 5               | False             | True             | 0.0215242                | 0.00113373                   |\n",
      "| 20001        | 10002         | 56               | 3000       | 5               | False             | True             | 0.0168267                | 0.00113373                   |\n",
      "| 20001        | 10003         | 56               | 3000       | 5               | False             | True             | 0.0232225                | 0.00113373                   |\n",
      "| 20001        | 10004         | 56               | 3000       | 5               | False             | True             | 0.0243948                | 0.00113373                   |\n",
      "| 20001        | 10005         | 56               | 3000       | 5               | False             | True             | 0.0140081                | 0.00113373                   |\n",
      "\n",
      "\n",
      "Datos Info\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9238176 entries, 35 to 9238140\n",
      "Data columns (total 46 columns):\n",
      " #   Column                      Dtype         \n",
      "---  ------                      -----         \n",
      " 0   product_id                  int64         \n",
      " 1   customer_id                 int64         \n",
      " 2   periodo                     datetime64[ns]\n",
      " 3   total_tn                    float64       \n",
      " 4   cat1                        object        \n",
      " 5   cat2                        object        \n",
      " 6   cat3                        object        \n",
      " 7   brand                       object        \n",
      " 8   sku_size                    int64         \n",
      " 9   descripcion                 object        \n",
      " 10  total_tn_scaled             float64       \n",
      " 11  total_tn_lag_1              float64       \n",
      " 12  total_tn_lag_2              float64       \n",
      " 13  total_tn_lag_3              float64       \n",
      " 14  total_tn_lag_4              float64       \n",
      " 15  total_tn_lag_5              float64       \n",
      " 16  total_tn_lag_6              float64       \n",
      " 17  total_tn_lag_7              float64       \n",
      " 18  total_tn_lag_8              float64       \n",
      " 19  total_tn_lag_9              float64       \n",
      " 20  total_tn_lag_10             float64       \n",
      " 21  total_tn_lag_11             float64       \n",
      " 22  total_tn_lag_12             float64       \n",
      " 23  delta_total_tn_lag_1        float64       \n",
      " 24  delta_total_tn_lag_2        float64       \n",
      " 25  delta_total_tn_lag_3        float64       \n",
      " 26  delta_total_tn_lag_4        float64       \n",
      " 27  delta_total_tn_lag_5        float64       \n",
      " 28  delta_total_tn_lag_6        float64       \n",
      " 29  delta_total_tn_lag_7        float64       \n",
      " 30  delta_total_tn_lag_8        float64       \n",
      " 31  delta_total_tn_lag_9        float64       \n",
      " 32  delta_total_tn_lag_10       float64       \n",
      " 33  delta_total_tn_lag_11       float64       \n",
      " 34  delta_total_tn_lag_12       float64       \n",
      " 35  target                      float64       \n",
      " 36  año                         int32         \n",
      " 37  mes                         int32         \n",
      " 38  quarter                     int32         \n",
      " 39  parent_product              int64         \n",
      " 40  sku_size_rank               float64       \n",
      " 41  is_smallest_sku             bool          \n",
      " 42  is_largest_sku              bool          \n",
      " 43  parent_product_sales        float64       \n",
      " 44  market_share_in_parent      float64       \n",
      " 45  avg_market_share_in_parent  float64       \n",
      "dtypes: bool(2), datetime64[ns](1), float64(31), int32(3), int64(4), object(5)\n",
      "memory usage: 3.0+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Agregacion de nuevas columnas al dataset\n",
    "\n",
    "# 1. Ordenar el DataFrame\n",
    "df = data_scaled.sort_values(['product_id', 'periodo', 'customer_id'])\n",
    "\n",
    "# 2. Agregar LAGS\n",
    "\n",
    "# Definir el número de lags\n",
    "num_lags = 12  # Puedes ajustar este valor según tus necesidades\n",
    "\n",
    "for i in range(1, num_lags+1): \n",
    "    df[f'total_tn_lag_{i}'] = df.groupby(['product_id', 'customer_id'])['total_tn_scaled'].shift(i)\n",
    "\n",
    "# 3. Agregar deltas de los LAGS\n",
    "for i in range(1, 13):\n",
    "    df[f'delta_total_tn_lag_{i}'] = df['total_tn_scaled'] - df[f'total_tn_lag_{i}']\n",
    "\n",
    "# 4. Agregar total_tn_estandarizado n+2\n",
    "df['target'] = df.groupby(['product_id', 'customer_id'])['total_tn_scaled'].shift(-2)\n",
    "\n",
    "# 5. Agregar año, mes y quarter\n",
    "df['periodo'] = pd.to_datetime(df['periodo'])\n",
    "df['año'] = df['periodo'].dt.year\n",
    "df['mes'] = df['periodo'].dt.month\n",
    "df['quarter'] = df['periodo'].dt.quarter\n",
    "\n",
    "# 6. Crear parent_product\n",
    "df['parent_product'] = df.groupby(['cat1', 'cat2', 'cat3', 'brand']).ngroup()\n",
    "\n",
    "# 7. Crear variables que indiquen la posición relativa del producto dentro de su familia\n",
    "df['sku_size_rank'] = df.groupby('parent_product')['sku_size'].rank(method='dense')\n",
    "df['is_smallest_sku'] = df['sku_size_rank'] == 1\n",
    "df['is_largest_sku'] = df.groupby('parent_product')['sku_size_rank'].transform('max') == df['sku_size_rank']\n",
    "\n",
    "# 8. Calcular la participación de mercado de cada SKU dentro de su parent_product\n",
    "df['parent_product_sales'] = df.groupby(['parent_product', 'periodo'])['total_tn_scaled'].transform('sum')\n",
    "df['market_share_in_parent'] = df['total_tn_scaled'] / df['parent_product_sales']\n",
    "df['avg_market_share_in_parent'] = df.groupby(['parent_product', 'product_id'])['market_share_in_parent'].transform('mean')\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame procesado\n",
    "print(\"\\n\\nColumnas actuales\\n\")\n",
    "print(df[['product_id','customer_id', 'parent_product', 'sku_size', 'sku_size_rank', 'is_smallest_sku', 'is_largest_sku', \n",
    "           'market_share_in_parent', 'avg_market_share_in_parent']].head(5).to_markdown(index=False, numalign='left', stralign='left'))\n",
    "\n",
    "# Mostrar información sobre el DataFrame procesado\n",
    "print(\"\\n\\nDatos Info\\n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hay un total de 40956736 valores nulos en el DataFrame.\n",
      "\n",
      "Valores nulos por columna:\n",
      "\n",
      "product_id                          0\n",
      "customer_id                         0\n",
      "periodo                             0\n",
      "total_tn                            0\n",
      "cat1                                0\n",
      "cat2                                0\n",
      "cat3                                0\n",
      "brand                               0\n",
      "sku_size                            0\n",
      "descripcion                         0\n",
      "total_tn_scaled                     0\n",
      "total_tn_lag_1                 256616\n",
      "total_tn_lag_2                 513232\n",
      "total_tn_lag_3                 769848\n",
      "total_tn_lag_4                1026464\n",
      "total_tn_lag_5                1283080\n",
      "total_tn_lag_6                1539696\n",
      "total_tn_lag_7                1796312\n",
      "total_tn_lag_8                2052928\n",
      "total_tn_lag_9                2309544\n",
      "total_tn_lag_10               2566160\n",
      "total_tn_lag_11               2822776\n",
      "total_tn_lag_12               3079392\n",
      "delta_total_tn_lag_1           256616\n",
      "delta_total_tn_lag_2           513232\n",
      "delta_total_tn_lag_3           769848\n",
      "delta_total_tn_lag_4          1026464\n",
      "delta_total_tn_lag_5          1283080\n",
      "delta_total_tn_lag_6          1539696\n",
      "delta_total_tn_lag_7          1796312\n",
      "delta_total_tn_lag_8          2052928\n",
      "delta_total_tn_lag_9          2309544\n",
      "delta_total_tn_lag_10         2566160\n",
      "delta_total_tn_lag_11         2822776\n",
      "delta_total_tn_lag_12         3079392\n",
      "target                         513232\n",
      "año                                 0\n",
      "mes                                 0\n",
      "quarter                             0\n",
      "parent_product                      0\n",
      "sku_size_rank                       0\n",
      "is_smallest_sku                     0\n",
      "is_largest_sku                      0\n",
      "parent_product_sales                0\n",
      "market_share_in_parent         411408\n",
      "avg_market_share_in_parent          0\n",
      "dtype: int64\n",
      "\n",
      "Hay un total de 166128859 valores iguales a cero en el DataFrame.\n",
      "\n",
      "Valores iguales a cero por columna:\n",
      "product_id                          0\n",
      "customer_id                         0\n",
      "periodo                             0\n",
      "total_tn                      6982654\n",
      "cat1                                0\n",
      "cat2                                0\n",
      "cat3                                0\n",
      "brand                               0\n",
      "sku_size                            0\n",
      "descripcion                         0\n",
      "total_tn_scaled               6982654\n",
      "total_tn_lag_1                6772857\n",
      "total_tn_lag_2                6582849\n",
      "total_tn_lag_3                6395056\n",
      "total_tn_lag_4                6211867\n",
      "total_tn_lag_5                6015537\n",
      "total_tn_lag_6                5831677\n",
      "total_tn_lag_7                5645864\n",
      "total_tn_lag_8                5451985\n",
      "total_tn_lag_9                5260126\n",
      "total_tn_lag_10               5076699\n",
      "total_tn_lag_11               4886035\n",
      "total_tn_lag_12               4683312\n",
      "delta_total_tn_lag_1          5784941\n",
      "delta_total_tn_lag_2          5666674\n",
      "delta_total_tn_lag_3          5481703\n",
      "delta_total_tn_lag_4          5298221\n",
      "delta_total_tn_lag_5          5114163\n",
      "delta_total_tn_lag_6          4942948\n",
      "delta_total_tn_lag_7          4767873\n",
      "delta_total_tn_lag_8          4594451\n",
      "delta_total_tn_lag_9          4420610\n",
      "delta_total_tn_lag_10         4264999\n",
      "delta_total_tn_lag_11         4103766\n",
      "delta_total_tn_lag_12         3928930\n",
      "target                        6580498\n",
      "año                                 0\n",
      "mes                                 0\n",
      "quarter                             0\n",
      "parent_product                  10116\n",
      "sku_size_rank                       0\n",
      "is_smallest_sku               5665212\n",
      "is_largest_sku                5741928\n",
      "parent_product_sales           411408\n",
      "market_share_in_parent        6571246\n",
      "avg_market_share_in_parent          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Control de Nulos y Ceros\n",
    "\n",
    "# Contar valores nulos en todo el DataFrame\n",
    "total_nulos = df.isnull().sum().sum()\n",
    "print(f'\\nHay un total de {total_nulos} valores nulos en el DataFrame.')\n",
    "\n",
    "# Contar valores nulos por columna\n",
    "nulos_por_columna = df.isnull().sum()\n",
    "print('\\nValores nulos por columna:\\n')\n",
    "print(nulos_por_columna)\n",
    "\n",
    "# Contar valores iguales a cero en todo el DataFrame\n",
    "total_ceros = (df == 0).sum().sum()\n",
    "print(f'\\nHay un total de {total_ceros} valores iguales a cero en el DataFrame.')\n",
    "\n",
    "# Contar valores iguales a cero por columna\n",
    "ceros_por_columna = (df == 0).sum()\n",
    "print('\\nValores iguales a cero por columna:')\n",
    "print(ceros_por_columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| product_id   | customer_id   | periodo             | total_tn   | cat1   | cat2   | cat3   | brand   | sku_size   | descripcion    | total_tn_scaled   | total_tn_lag_1   | total_tn_lag_2   | total_tn_lag_3   | total_tn_lag_4   | total_tn_lag_5   | total_tn_lag_6   | total_tn_lag_7   | total_tn_lag_8   | total_tn_lag_9   | total_tn_lag_10   | total_tn_lag_11   | total_tn_lag_12   | delta_total_tn_lag_1   | delta_total_tn_lag_2   | delta_total_tn_lag_3   | delta_total_tn_lag_4   | delta_total_tn_lag_5   | delta_total_tn_lag_6   | delta_total_tn_lag_7   | delta_total_tn_lag_8   | delta_total_tn_lag_9   | delta_total_tn_lag_10   | delta_total_tn_lag_11   | delta_total_tn_lag_12   | target   | año   | mes   | quarter   | parent_product   | sku_size_rank   | is_smallest_sku   | is_largest_sku   | parent_product_sales   | market_share_in_parent   | avg_market_share_in_parent   |\n",
      "|:-------------|:--------------|:--------------------|:-----------|:-------|:-------|:-------|:--------|:-----------|:---------------|:------------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:------------------|:------------------|:------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:------------------------|:------------------------|:------------------------|:---------|:------|:------|:----------|:-----------------|:----------------|:------------------|:-----------------|:-----------------------|:-------------------------|:-----------------------------|\n",
      "| 21276        | 10428         | 2019-12-01 00:00:00 | 0          | PC     | PIEL1  | Cara   | NIVEA   | 140        | reconstruccion | 0                 | 0                | 0                | 0                | 0                | 0.00222752       | 0                | 0.00445008       | 0                | 0                | 0                 | 0                 | 0                 | 0                      | 0                      | 0                      | 0                      | -0.00222752            | 0                      | -0.00445008            | 0                      | 0                      | 0                       | 0                       | 0                       | nan      | 2019  | 12    | 4         | 114              | 1               | True              | False            | 0.0186089              | 0                        | 0.0130822                    |\n",
      "| 21276        | 10456         | 2019-12-01 00:00:00 | 0          | PC     | PIEL1  | Cara   | NIVEA   | 140        | reconstruccion | 0                 | 0                | 0                | 0.00147891       | 0                | 0                | 0                | 0                | 0                | 0.00887054       | 0                 | 0                 | 0                 | 0                      | 0                      | -0.00147891            | 0                      | 0                      | 0                      | 0                      | 0                      | -0.00887054            | 0                       | 0                       | 0                       | nan      | 2019  | 12    | 4         | 114              | 1               | True              | False            | 0.0186089              | 0                        | 0.0130822                    |\n",
      "| 21276        | 10462         | 2019-12-01 00:00:00 | 0          | PC     | PIEL1  | Cara   | NIVEA   | 140        | reconstruccion | 0                 | 0                | 0.000749719      | 0                | 0                | 0                | 0                | 0.00147891       | 0                | 0.000749719      | 0                 | 0                 | 0                 | 0                      | -0.000749719           | 0                      | 0                      | 0                      | 0                      | -0.00147891            | 0                      | -0.000749719           | 0                       | 0                       | 0                       | nan      | 2019  | 12    | 4         | 114              | 1               | True              | False            | 0.0186089              | 0                        | 0.0130822                    |\n",
      "| 21276        | 10495         | 2019-12-01 00:00:00 | 0          | PC     | PIEL1  | Cara   | NIVEA   | 140        | reconstruccion | 0                 | 0                | 0                | 0                | 0                | 0                | 0                | 0                | 0                | 0.00147891       | 0                 | 0                 | 0                 | 0                      | 0                      | 0                      | 0                      | 0                      | 0                      | 0                      | 0                      | -0.00147891            | 0                       | 0                       | 0                       | nan      | 2019  | 12    | 4         | 114              | 1               | True              | False            | 0.0186089              | 0                        | 0.0130822                    |\n",
      "| 21276        | 10550         | 2019-12-01 00:00:00 | 0          | PC     | PIEL1  | Cara   | NIVEA   | 140        | reconstruccion | 0                 | 0.00370313       | 0                | 0.000749719      | 0                | 0                | 0                | 0                | 0                | 0                | 0                 | 0                 | 0                 | -0.00370313            | 0                      | -0.000749719           | 0                      | 0                      | 0                      | 0                      | 0                      | 0                      | 0                       | 0                       | 0                       | nan      | 2019  | 12    | 4         | 114              | 1               | True              | False            | 0.0186089              | 0                        | 0.0130822                    |\n"
     ]
    }
   ],
   "source": [
    "print(df.tail().to_markdown(index=False, numalign='left', stralign='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periodos con NaN en 'target':\n",
      "<DatetimeArray>\n",
      "['2019-11-01 00:00:00', '2019-12-01 00:00:00']\n",
      "Length: 2, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Suponiendo que 'target' es la columna donde quieres verificar los NaN\n",
    "periodos_con_nan = df[df['target'].isna()]['periodo'].unique()\n",
    "\n",
    "print(\"Periodos con NaN en 'target':\")\n",
    "print(periodos_con_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de product_id diferentes es: 780\n"
     ]
    }
   ],
   "source": [
    "# Contar el número de product_id únicos\n",
    "num_productos_diferentes = df['product_id'].nunique()\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(f\"El número de product_id diferentes es: {num_productos_diferentes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciona las características y el target\n",
    "features = [col for col in df.columns if col not in ['descripcion']]\n",
    "data = df[features]\n",
    "\n",
    "# Parámetros de división\n",
    "test_months = 2 \n",
    "val_months = 2 \n",
    "\n",
    "# Fechas de corte (Usamos la columna 'fecha' del dataset)\n",
    "test_start_date = data['periodo'].max() - pd.DateOffset(months=test_months)\n",
    "val_start_date = test_start_date - pd.DateOffset(months=val_months)\n",
    "\n",
    "# Máscaras para seleccionar conjuntos \n",
    "train_mask = data['periodo'] < val_start_date\n",
    "val_mask = (data['periodo'] >= val_start_date) & (data['periodo'] < test_start_date)\n",
    "test_mask = data['periodo'] >= test_start_date\n",
    "\n",
    "# División de datos\n",
    "X_train = data.drop(['target', 'periodo', 'total_tn'], axis=1)[train_mask]\n",
    "y_train = data['target'][train_mask]\n",
    "weights_train = data['total_tn'][train_mask]\n",
    "\n",
    "X_val = data.drop(['target', 'periodo', 'total_tn'], axis=1)[val_mask]\n",
    "y_val = data['target'][val_mask]\n",
    "weights_val = data['total_tn'][val_mask]\n",
    "\n",
    "X_test = data.drop(['target', 'periodo', 'total_tn'], axis=1)[test_mask]\n",
    "y_test = data['target'][test_mask]\n",
    "weights_test = data['total_tn'][test_mask]\n",
    "\n",
    "prediction_data = data[test_mask].copy()\n",
    "\n",
    "# Columnas categóricas\n",
    "\n",
    "# Columnas categóricas tus DataFrames X_train, X_val, X_test\n",
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand']\n",
    "\n",
    "# Crear un codificador para cada columna categórica\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    encoders[col] = LabelEncoder()\n",
    "    X_train[col] = encoders[col].fit_transform(X_train[col])\n",
    "\n",
    "# Transformar los conjuntos de validación y prueba (usando los mismos codificadores)\n",
    "for col in categorical_cols:\n",
    "    X_val[col] = encoders[col].transform(X_val[col])\n",
    "    X_test[col] = encoders[col].transform(X_test[col])\n",
    "    prediction_data[col] = encoders[col].transform(prediction_data[col])\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| product_id   | customer_id   | cat1   | cat2   | cat3   | brand   | sku_size   | total_tn_scaled   | total_tn_lag_1   | total_tn_lag_2   | total_tn_lag_3   | total_tn_lag_4   | total_tn_lag_5   | total_tn_lag_6   | total_tn_lag_7   | total_tn_lag_8   | total_tn_lag_9   | total_tn_lag_10   | total_tn_lag_11   | total_tn_lag_12   | delta_total_tn_lag_1   | delta_total_tn_lag_2   | delta_total_tn_lag_3   | delta_total_tn_lag_4   | delta_total_tn_lag_5   | delta_total_tn_lag_6   | delta_total_tn_lag_7   | delta_total_tn_lag_8   | delta_total_tn_lag_9   | delta_total_tn_lag_10   | delta_total_tn_lag_11   | delta_total_tn_lag_12   | año   | mes   | quarter   | parent_product   | sku_size_rank   | is_smallest_sku   | is_largest_sku   | parent_product_sales   | market_share_in_parent   | avg_market_share_in_parent   |\n",
      "|:-------------|:--------------|:-------|:-------|:-------|:--------|:-----------|:------------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:------------------|:------------------|:------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:------------------------|:------------------------|:------------------------|:------|:------|:----------|:-----------------|:----------------|:------------------|:-----------------|:-----------------------|:-------------------------|:-----------------------------|\n",
      "| 20001        | 10001         | 1      | 10     | 47     | 0       | 3000       | 5.17632           | 4.70096          | 3.54501          | 4.98215          | 4.20356          | 6.08883          | 5.90091          | 4.87938          | 5.73949          | 5.95999          | 5.54371           | 5.48904           | 5.5062            | 0.475361               | 1.63131                | 0.19417                | 0.972755               | -0.912515              | -0.724594              | 0.296937               | -0.563173              | -0.783674              | -0.367388               | -0.31272                | -0.329883               | 2019  | 10    | 4         | 56               | 5               | False             | True             | 323.925                | 0.01598                  | 0.00113373                   |\n",
      "| 20001        | 10002         | 1      | 10     | 47     | 0       | 3000       | 2.91279           | 4.29163          | 0                | 3.64143          | 4.97722          | 3.46176          | 4.03277          | 3.49562          | 2.01304          | 2.37764          | 4.88499           | 3.32809           | 3.03413           | -1.37884               | 2.91279                | -0.728636              | -2.06443               | -0.54897               | -1.11998               | -0.582833              | 0.899751               | 0.535149               | -1.9722                 | -0.415302               | -0.121342               | 2019  | 10    | 4         | 56               | 5               | False             | True             | 323.925                | 0.00899218               | 0.00113373                   |\n",
      "| 20001        | 10003         | 1      | 10     | 47     | 0       | 3000       | 4.34389           | 5.4562           | 5.09419          | 4.93616          | 0                | 1.04674          | 5.44242          | 5.14691          | 4.40934          | 5.20131          | 4.60432           | 5.30577           | 4.22525           | -1.11231               | -0.750305              | -0.592269              | 4.34389                | 3.29715                | -1.09854               | -0.803022              | -0.0654539             | -0.857428              | -0.260429               | -0.961879               | 0.118633                | 2019  | 10    | 4         | 56               | 5               | False             | True             | 323.925                | 0.0134102                | 0.00113373                   |\n",
      "| 20001        | 10004         | 1      | 10     | 47     | 0       | 3000       | 5.78678           | 5.66761          | 4.57589          | 5.43644          | 4.21538          | 5.96621          | 4.52913          | 4.64097          | 5.05153          | 4.26601          | 5.47139           | 4.57589           | 6.15371           | 0.11917                | 1.21089                | 0.350338               | 1.5714                 | -0.179435              | 1.25765                | 1.14581                | 0.735252               | 1.52077                | 0.315392                | 1.21089                 | -0.366926               | 2019  | 10    | 4         | 56               | 5               | False             | True             | 323.925                | 0.0178646                | 0.00113373                   |\n",
      "| 20001        | 10005         | 1      | 10     | 47     | 0       | 3000       | 2.89808           | 2.62496          | 0                | 2.22527          | 0                | 2.82706          | 3.1433           | 2.06692          | 1.71546          | 2.019            | 3.01344           | 2.81237           | 3.56866           | 0.273117               | 2.89808                | 0.672809               | 2.89808                | 0.071012               | -0.245221              | 0.831151               | 1.18261                | 0.879072               | -0.11536                | 0.0857059               | -0.670579               | 2019  | 10    | 4         | 56               | 5               | False             | True             | 323.925                | 0.00894676               | 0.00113373                   |\n"
     ]
    }
   ],
   "source": [
    "print(X_test.head().to_markdown(index=False, numalign='left', stralign='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| product_id   | customer_id   | periodo             | total_tn   | cat1   | cat2   | cat3   | brand   | sku_size   | total_tn_scaled   | total_tn_lag_1   | total_tn_lag_2   | total_tn_lag_3   | total_tn_lag_4   | total_tn_lag_5   | total_tn_lag_6   | total_tn_lag_7   | total_tn_lag_8   | total_tn_lag_9   | total_tn_lag_10   | total_tn_lag_11   | total_tn_lag_12   | delta_total_tn_lag_1   | delta_total_tn_lag_2   | delta_total_tn_lag_3   | delta_total_tn_lag_4   | delta_total_tn_lag_5   | delta_total_tn_lag_6   | delta_total_tn_lag_7   | delta_total_tn_lag_8   | delta_total_tn_lag_9   | delta_total_tn_lag_10   | delta_total_tn_lag_11   | delta_total_tn_lag_12   | target   | año   | mes   | quarter   | parent_product   | sku_size_rank   | is_smallest_sku   | is_largest_sku   | parent_product_sales   | market_share_in_parent   | avg_market_share_in_parent   |\n",
      "|:-------------|:--------------|:--------------------|:-----------|:-------|:-------|:-------|:--------|:-----------|:------------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:------------------|:------------------|:------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:------------------------|:------------------------|:------------------------|:---------|:------|:------|:----------|:-----------------|:----------------|:------------------|:-----------------|:-----------------------|:-------------------------|:-----------------------------|\n",
      "| 20001        | 10001         | 2019-10-01 00:00:00 | 176.03     | 1      | 10     | 47     | 0       | 3000       | 5.17632           | 4.70096          | 3.54501          | 4.98215          | 4.20356          | 6.08883          | 5.90091          | 4.87938          | 5.73949          | 5.95999          | 5.54371           | 5.48904           | 5.5062            | 0.475361               | 1.63131                | 0.19417                | 0.972755               | -0.912515              | -0.724594              | 0.296937               | -0.563173              | -0.783674              | -0.367388               | -0.31272                | -0.329883               | 5.19971  | 2019  | 10    | 4         | 56               | 5               | False             | True             | 323.925                | 0.01598                  | 0.00113373                   |\n",
      "| 20001        | 10002         | 2019-10-01 00:00:00 | 17.4081    | 1      | 10     | 47     | 0       | 3000       | 2.91279           | 4.29163          | 0                | 3.64143          | 4.97722          | 3.46176          | 4.03277          | 3.49562          | 2.01304          | 2.37764          | 4.88499           | 3.32809           | 3.03413           | -1.37884               | 2.91279                | -0.728636              | -2.06443               | -0.54897               | -1.11998               | -0.582833              | 0.899751               | 0.535149               | -1.9722                 | -0.415302               | -0.121342               | 4.7391   | 2019  | 10    | 4         | 56               | 5               | False             | True             | 323.925                | 0.00899218               | 0.00113373                   |\n",
      "| 20001        | 10003         | 2019-10-01 00:00:00 | 76.0062    | 1      | 10     | 47     | 0       | 3000       | 4.34389           | 5.4562           | 5.09419          | 4.93616          | 0                | 1.04674          | 5.44242          | 5.14691          | 4.40934          | 5.20131          | 4.60432           | 5.30577           | 4.22525           | -1.11231               | -0.750305              | -0.592269              | 4.34389                | 3.29715                | -1.09854               | -0.803022              | -0.0654539             | -0.857428              | -0.260429               | -0.961879               | 0.118633                | 4.6374   | 2019  | 10    | 4         | 56               | 5               | False             | True             | 323.925                | 0.0134102                | 0.00113373                   |\n",
      "| 20001        | 10004         | 2019-10-01 00:00:00 | 324.962    | 1      | 10     | 47     | 0       | 3000       | 5.78678           | 5.66761          | 4.57589          | 5.43644          | 4.21538          | 5.96621          | 4.52913          | 4.64097          | 5.05153          | 4.26601          | 5.47139           | 4.57589           | 6.15371           | 0.11917                | 1.21089                | 0.350338               | 1.5714                 | -0.179435              | 1.25765                | 1.14581                | 0.735252               | 1.52077                | 0.315392                | 1.21089                 | -0.366926               | 3.5737   | 2019  | 10    | 4         | 56               | 5               | False             | True             | 323.925                | 0.0178646                | 0.00113373                   |\n",
      "| 20001        | 10005         | 2019-10-01 00:00:00 | 17.1392    | 1      | 10     | 47     | 0       | 3000       | 2.89808           | 2.62496          | 0                | 2.22527          | 0                | 2.82706          | 3.1433           | 2.06692          | 1.71546          | 2.019            | 3.01344           | 2.81237           | 3.56866           | 0.273117               | 2.89808                | 0.672809               | 2.89808                | 0.071012               | -0.245221              | 0.831151               | 1.18261                | 0.879072               | -0.11536                | 0.0857059               | -0.670579               | 3.02547  | 2019  | 10    | 4         | 56               | 5               | False             | True             | 323.925                | 0.00894676               | 0.00113373                   |\n"
     ]
    }
   ],
   "source": [
    "print(prediction_data.head().to_markdown(index=False, numalign='left', stralign='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train, label=y_train, weight=weights_train, feature_name=list(X_train.columns), categorical_feature=categorical_cols)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, weight=weights_val, feature_name=list(X_val.columns), categorical_feature=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l2: 1.06872\tvalid_0's huber: 0.436037\n",
      "[200]\tvalid_0's l2: 0.768422\tvalid_0's huber: 0.322988\n",
      "[300]\tvalid_0's l2: 0.689983\tvalid_0's huber: 0.288922\n",
      "[400]\tvalid_0's l2: 0.66505\tvalid_0's huber: 0.277348\n",
      "[500]\tvalid_0's l2: 0.655874\tvalid_0's huber: 0.27301\n",
      "[600]\tvalid_0's l2: 0.651108\tvalid_0's huber: 0.270615\n",
      "[700]\tvalid_0's l2: 0.648765\tvalid_0's huber: 0.269281\n",
      "[800]\tvalid_0's l2: 0.647382\tvalid_0's huber: 0.268353\n",
      "Early stopping, best iteration is:\n",
      "[847]\tvalid_0's l2: 0.647105\tvalid_0's huber: 0.268171\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective': 'regression',  # Tipo de problema: 'regression', 'binary', 'multiclass'\n",
    "    'metric': ['mse', 'huber'],  # Métricas de evaluación. Opciones: 'mse', 'rmse', 'mae', 'mape', 'huber'\n",
    "    'num_leaves': 283,  # Número máximo de hojas en cada árbol. Rango típico: 20-100\n",
    "    'max_depth': -1,  # Profundidad máxima del árbol. -1 para sin límite. Rango típico: 3-12\n",
    "    'learning_rate': 0.008,  # Tasa de aprendizaje. Rango típico: 0.01-0.3\n",
    "    'feature_fraction': 0.67,  # Fracción de características usadas en cada iteración. Rango: 0.5-1.0\n",
    "    'bagging_fraction': 0.88,  # Fracción de datos usados para cada iteración. Rango: 0.5-1.0\n",
    "    'bagging_freq': 8,  # Frecuencia de bagging. 0 significa deshabilitar bagging. Valor típico: 5\n",
    "    'lambda_l1': 0.21, \n",
    "    'lambda_l2': 0.67, \n",
    "    'min_child_samples': 36,  # Número mínimo de muestras en un nodo hoja. Rango típico: 10-50\n",
    "    'reg_alpha': 0.01,  # Regularización L1. Rango típico: 0-1\n",
    "    'reg_lambda': 0.01,  # Regularización L2. Rango típico: 0-1\n",
    "    'min_split_gain': 0.1,  # Ganancia mínima para realizar una división. Rango típico: 0-0.5\n",
    "    'max_bin': 255,  # Número máximo de bins para variables continuas. Rango típico: 200-1000\n",
    "    'boosting': 'gbdt',  # Tipo de boosting. Opciones: 'gbdt', 'dart', 'goss'\n",
    "    'verbose': -1,  # Nivel de detalle en la salida. -1: nada, 0: errores, 1: advertencias, >1: info\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 1,\n",
    "    'device': 'gpu',  # Dispositivo a utilizar: 'cpu' o 'gpu'\n",
    "}\n",
    "\n",
    "num_boost_round = 1000  # Número máximo de iteraciones de boosting\n",
    "\n",
    "# Callbacks para early stopping y logging\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=50),  # Detiene el entrenamiento si no hay mejora en 50 rondas\n",
    "    lgb.log_evaluation(period=100)  # Registra la evaluación cada 100 iteraciones\n",
    "]\n",
    "\n",
    "model = lgb.train(\n",
    "    params, \n",
    "    train_data, \n",
    "    num_boost_round, \n",
    "    valid_sets=[val_data],\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.1463668165429294\n",
      "R2 Score: 0.6065366651089463\n"
     ]
    }
   ],
   "source": [
    "# Predicciones en el conjunto de validación\n",
    "val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "\n",
    "# Calcula métricas\n",
    "mse = mean_squared_error(y_val, val_preds)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val, val_preds)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| product_id   | customer_id   | periodo             | total_tn   | cat1   | cat2   | cat3   | brand   | sku_size   | total_tn_scaled   | total_tn_lag_1   | total_tn_lag_2   | total_tn_lag_3   | total_tn_lag_4   | total_tn_lag_5   | total_tn_lag_6   | total_tn_lag_7   | total_tn_lag_8   | total_tn_lag_9   | total_tn_lag_10   | total_tn_lag_11   | total_tn_lag_12   | delta_total_tn_lag_1   | delta_total_tn_lag_2   | delta_total_tn_lag_3   | delta_total_tn_lag_4   | delta_total_tn_lag_5   | delta_total_tn_lag_6   | delta_total_tn_lag_7   | delta_total_tn_lag_8   | delta_total_tn_lag_9   | delta_total_tn_lag_10   | delta_total_tn_lag_11   | delta_total_tn_lag_12   | target   | año   | mes   | quarter   | parent_product   | sku_size_rank   | is_smallest_sku   | is_largest_sku   | parent_product_sales   | market_share_in_parent   | avg_market_share_in_parent   |\n",
      "|:-------------|:--------------|:--------------------|:-----------|:-------|:-------|:-------|:--------|:-----------|:------------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:------------------|:------------------|:------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:------------------------|:------------------------|:------------------------|:---------|:------|:------|:----------|:-----------------|:----------------|:------------------|:-----------------|:-----------------------|:-------------------------|:-----------------------------|\n",
      "| 21276        | 10428         | 2019-12-01 00:00:00 | 0          | 2      | 6      | 18     | 23      | 140        | 0                 | 0                | 0                | 0                | 0                | 0.00222752       | 0                | 0.00445008       | 0                | 0                | 0                 | 0                 | 0                 | 0                      | 0                      | 0                      | 0                      | -0.00222752            | 0                      | -0.00445008            | 0                      | 0                      | 0                       | 0                       | 0                       | nan      | 2019  | 12    | 4         | 114              | 1               | True              | False            | 0.0186089              | 0                        | 0.0130822                    |\n",
      "| 21276        | 10456         | 2019-12-01 00:00:00 | 0          | 2      | 6      | 18     | 23      | 140        | 0                 | 0                | 0                | 0.00147891       | 0                | 0                | 0                | 0                | 0                | 0.00887054       | 0                 | 0                 | 0                 | 0                      | 0                      | -0.00147891            | 0                      | 0                      | 0                      | 0                      | 0                      | -0.00887054            | 0                       | 0                       | 0                       | nan      | 2019  | 12    | 4         | 114              | 1               | True              | False            | 0.0186089              | 0                        | 0.0130822                    |\n",
      "| 21276        | 10462         | 2019-12-01 00:00:00 | 0          | 2      | 6      | 18     | 23      | 140        | 0                 | 0                | 0.000749719      | 0                | 0                | 0                | 0                | 0.00147891       | 0                | 0.000749719      | 0                 | 0                 | 0                 | 0                      | -0.000749719           | 0                      | 0                      | 0                      | 0                      | -0.00147891            | 0                      | -0.000749719           | 0                       | 0                       | 0                       | nan      | 2019  | 12    | 4         | 114              | 1               | True              | False            | 0.0186089              | 0                        | 0.0130822                    |\n",
      "| 21276        | 10495         | 2019-12-01 00:00:00 | 0          | 2      | 6      | 18     | 23      | 140        | 0                 | 0                | 0                | 0                | 0                | 0                | 0                | 0                | 0                | 0.00147891       | 0                 | 0                 | 0                 | 0                      | 0                      | 0                      | 0                      | 0                      | 0                      | 0                      | 0                      | -0.00147891            | 0                       | 0                       | 0                       | nan      | 2019  | 12    | 4         | 114              | 1               | True              | False            | 0.0186089              | 0                        | 0.0130822                    |\n",
      "| 21276        | 10550         | 2019-12-01 00:00:00 | 0          | 2      | 6      | 18     | 23      | 140        | 0                 | 0.00370313       | 0                | 0.000749719      | 0                | 0                | 0                | 0                | 0                | 0                | 0                 | 0                 | 0                 | -0.00370313            | 0                      | -0.000749719           | 0                      | 0                      | 0                      | 0                      | 0                      | 0                      | 0                       | 0                       | 0                       | nan      | 2019  | 12    | 4         | 114              | 1               | True              | False            | 0.0186089              | 0                        | 0.0130822                    |\n"
     ]
    }
   ],
   "source": [
    "prediction_data_final = prediction_data[prediction_data['periodo'] == '2019-12-01 00:00:00'].copy()\n",
    "print(prediction_data_final.tail().to_markdown(index=False, numalign='left', stralign='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = prediction_data_final.drop(['target','periodo', 'total_tn'], axis=1)\n",
    "\n",
    "predictions = model.predict(X_pred, num_iteration=model.best_iteration)\n",
    "\n",
    "# Añade las predicciones al DataFrame de predicción\n",
    "prediction_data_final['predicted_tn'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| product_id   | customer_id   | periodo             | total_tn   | cat1   | cat2   | cat3   | brand   | sku_size   | total_tn_scaled   | total_tn_lag_1   | total_tn_lag_2   | total_tn_lag_3   | total_tn_lag_4   | total_tn_lag_5   | total_tn_lag_6   | total_tn_lag_7   | total_tn_lag_8   | total_tn_lag_9   | total_tn_lag_10   | total_tn_lag_11   | total_tn_lag_12   | delta_total_tn_lag_1   | delta_total_tn_lag_2   | delta_total_tn_lag_3   | delta_total_tn_lag_4   | delta_total_tn_lag_5   | delta_total_tn_lag_6   | delta_total_tn_lag_7   | delta_total_tn_lag_8   | delta_total_tn_lag_9   | delta_total_tn_lag_10   | delta_total_tn_lag_11   | delta_total_tn_lag_12   | target   | año   | mes   | quarter   | parent_product   | sku_size_rank   | is_smallest_sku   | is_largest_sku   | parent_product_sales   | market_share_in_parent   | avg_market_share_in_parent   | predicted_tn   |\n",
      "|:-------------|:--------------|:--------------------|:-----------|:-------|:-------|:-------|:--------|:-----------|:------------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:------------------|:------------------|:------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:-----------------------|:------------------------|:------------------------|:------------------------|:---------|:------|:------|:----------|:-----------------|:----------------|:------------------|:-----------------|:-----------------------|:-------------------------|:-----------------------------|:---------------|\n",
      "| 20001        | 10001         | 2019-12-01 00:00:00 | 180.219    | 1      | 10     | 47     | 0       | 3000       | 5.19971           | 5.47082          | 5.17632          | 4.70096          | 3.54501          | 4.98215          | 4.20356          | 6.08883          | 5.90091          | 4.87938          | 5.73949           | 5.95999           | 5.54371           | -0.271114              | 0.0233903              | 0.498751               | 1.6547                 | 0.217561               | 0.996145               | -0.889124              | -0.701204              | 0.320327               | -0.539783               | -0.760283               | -0.343998               | nan      | 2019  | 12    | 4         | 56               | 5               | False             | True             | 318.198                | 0.0163411                | 0.00113373                   | 4.99474        |\n",
      "| 20001        | 10002         | 2019-12-01 00:00:00 | 113.332    | 1      | 10     | 47     | 0       | 3000       | 4.7391            | 3.84192          | 2.91279          | 4.29163          | 0                | 3.64143          | 4.97722          | 3.46176          | 4.03277          | 3.49562          | 2.01304           | 2.37764           | 4.88499           | 0.897182               | 1.82631                | 0.447473               | 4.7391                 | 1.09768                | -0.238114              | 1.27734                | 0.706337               | 1.24348                | 2.72607                 | 2.36146                 | -0.145883               | nan      | 2019  | 12    | 4         | 56               | 5               | False             | True             | 318.198                | 0.0148936                | 0.00113373                   | 3.57882        |\n",
      "| 20001        | 10003         | 2019-12-01 00:00:00 | 102.275    | 1      | 10     | 47     | 0       | 3000       | 4.6374            | 4.46756          | 4.34389          | 5.4562           | 5.09419          | 4.93616          | 0                | 1.04674          | 5.44242          | 5.14691          | 4.40934           | 5.20131           | 4.60432           | 0.169833               | 0.29351                | -0.818801              | -0.456795              | -0.298758              | 4.6374                 | 3.59066                | -0.805027              | -0.509512              | 0.228057                | -0.563917               | 0.0330818               | nan      | 2019  | 12    | 4         | 56               | 5               | False             | True             | 318.198                | 0.014574                 | 0.00113373                   | 3.17201        |\n",
      "| 20001        | 10004         | 2019-12-01 00:00:00 | 34.6481    | 1      | 10     | 47     | 0       | 3000       | 3.5737            | 5.28157          | 5.78678          | 5.66761          | 4.57589          | 5.43644          | 4.21538          | 5.96621          | 4.52913          | 4.64097          | 5.05153           | 4.26601           | 5.47139           | -1.70787               | -2.21308               | -2.09391               | -1.00219               | -1.86275               | -0.641681              | -2.39252               | -0.955435              | -1.06727               | -1.47783                | -0.692319               | -1.89769                | nan      | 2019  | 12    | 4         | 56               | 5               | False             | True             | 318.198                | 0.0112311                | 0.00113373                   | 4.56239        |\n",
      "| 20001        | 10005         | 2019-12-01 00:00:00 | 19.6037    | 1      | 10     | 47     | 0       | 3000       | 3.02547           | 2.58184          | 2.89808          | 2.62496          | 0                | 2.22527          | 0                | 2.82706          | 3.1433           | 2.06692          | 1.71546           | 2.019             | 3.01344           | 0.443626               | 0.127394               | 0.400511               | 3.02547                | 0.800203               | 3.02547                | 0.198406               | -0.117827              | 0.958545               | 1.31001                 | 1.00647                 | 0.0120336               | nan      | 2019  | 12    | 4         | 56               | 5               | False             | True             | 318.198                | 0.00950815               | 0.00113373                   | 2.12653        |\n"
     ]
    }
   ],
   "source": [
    "print(prediction_data_final.head().to_markdown(index=False, numalign='left', stralign='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metodo de des-escalado:\n",
      "log\n",
      "\n",
      "¡Archivo listo!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
    "\n",
    "def inverse_scale_data(df, original_df, method):\n",
    "    def inverse_scale_group(group, original_group):\n",
    "        if method == 'standard' or method == 'robust' or method == 'quantile':\n",
    "            # Usar la relación entre los datos originales y escalados\n",
    "            scale_factor = original_group['total_tn'].mean() / group['total_tn_scaled'].mean()\n",
    "            return group['predicted_tn'] * scale_factor\n",
    "        elif method == 'log':\n",
    "            return np.expm1(group['predicted_tn'])\n",
    "        elif method == 'yeo-johnson':\n",
    "            pt = PowerTransformer(method='yeo-johnson')\n",
    "            pt.fit(original_group['total_tn'].values.reshape(-1, 1))\n",
    "            return pt.inverse_transform(group['predicted_tn'].values.reshape(-1, 1)).flatten()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    # Aplicar la transformación inversa a cada grupo\n",
    "    df['target_unscaled'] = df.groupby(['customer_id', 'product_id'])['predicted_tn'].transform(\n",
    "        lambda x: np.expm1(x) if method == 'log' else x\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Asumiendo que 'method' es el método que usaste para escalar originalmente\n",
    "#method = 'standard'  # Cambia esto al método que hayas usado\n",
    "\n",
    "print(\"Metodo de des-escalado:\")\n",
    "print(method)\n",
    "\n",
    "# Aplicar la transformación inversa\n",
    "prediction_data = inverse_scale_data(prediction_data_final, data_scaled, method)\n",
    "\n",
    "# Agrupar por ID y sumar los valores desnormalizados\n",
    "result = prediction_data.groupby('product_id')['target_unscaled'].sum().reset_index()\n",
    "\n",
    "# Renombrar la columna 'target_unscaled' a 'tn'\n",
    "result = result.rename(columns={'target_unscaled': 'tn'})\n",
    "\n",
    "# Guardar el resultado en un CSV\n",
    "result.to_csv('target_unscaled_sum_by_id_12_07_v9.csv', index=False)\n",
    "\n",
    "print(\"\\n¡Archivo listo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   feature  importance\n",
      "1              customer_id       11978\n",
      "20    delta_total_tn_lag_1        9536\n",
      "21    delta_total_tn_lag_2        9055\n",
      "40  market_share_in_parent        8576\n",
      "22    delta_total_tn_lag_3        8286\n",
      "23    delta_total_tn_lag_4        8084\n",
      "39    parent_product_sales        8001\n",
      "24    delta_total_tn_lag_5        7458\n",
      "8           total_tn_lag_1        7195\n",
      "25    delta_total_tn_lag_6        7193\n"
     ]
    }
   ],
   "source": [
    "feature_importance = model.feature_importance()\n",
    "feature_names = model.feature_name()\n",
    "\n",
    "# Crea un DataFrame con la importancia de las características\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "####################   RadomSearch   ####################\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "36 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 1173, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 954, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\engine.py\", line 282, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\basic.py\", line 3627, in __init__\n",
      "    train_set.construct()\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\basic.py\", line 2566, in construct\n",
      "    self._lazy_init(\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\basic.py\", line 2158, in _lazy_init\n",
      "    self.__init_from_np2d(data, params_str, ref_dataset)\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\basic.py\", line 2293, in __init_from_np2d\n",
      "    _safe_call(\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\basic.py\", line 294, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))\n",
      "lightgbm.basic.LightGBMError: Check failed: (feature_fraction) <= (1.0) at D:\\a\\1\\s\\lightgbm-python\\src\\io\\config_auto.cpp, line 382 .\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 1173, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 954, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\engine.py\", line 282, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\basic.py\", line 3627, in __init__\n",
      "    train_set.construct()\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\basic.py\", line 2566, in construct\n",
      "    self._lazy_init(\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\basic.py\", line 2158, in _lazy_init\n",
      "    self.__init_from_np2d(data, params_str, ref_dataset)\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\basic.py\", line 2293, in __init_from_np2d\n",
      "    _safe_call(\n",
      "  File \"c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\lightgbm\\basic.py\", line 294, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))\n",
      "lightgbm.basic.LightGBMError: Check failed: (bagging_fraction) <= (1.0) at D:\\a\\1\\s\\lightgbm-python\\src\\io\\config_auto.cpp, line 366 .\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Usuario\\OneDrive\\Carpeta ONE DRIVE\\Maestria\\17 Labo III - Series de Tiempo\\Labo3-Guille\\ve-Labo3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1052: UserWarning: One or more of the test scores are non-finite: [       nan 0.58894662 0.54624393 0.47837766        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.60065596 0.53829198        nan        nan        nan\n",
      " 0.5331656  0.58193963        nan        nan 0.17304617 0.588907\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.58615303        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.56871776 0.58940532\n",
      "        nan 0.57096587        nan        nan        nan        nan\n",
      "        nan 0.54603132]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.493855\n",
      "RMSE: 0.13911906837115037\n",
      "R2 Score: 0.6445386910477233\n",
      "Mejores parámetros: {'bagging_fraction': 0.8125716742746938, 'bagging_freq': 7, 'feature_fraction': 0.9901480892728447, 'lambda_l1': 0.5632755719763837, 'lambda_l2': 0.6955160864261275, 'learning_rate': 0.12146516352470055, 'min_child_samples': 19, 'num_leaves': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "# Definir el espacio de búsqueda de hiperparámetros (esto permanece igual)\n",
    "param_dist = {\n",
    "    'num_leaves': randint(30, 300),\n",
    "    'learning_rate': uniform(0.01, 0.8),\n",
    "    'feature_fraction': uniform(0.8, 0.4),\n",
    "    'bagging_fraction': uniform(0.8, 0.4),\n",
    "    'bagging_freq': randint(1, 10),\n",
    "    'min_child_samples': randint(5, 100),\n",
    "    'lambda_l1': uniform(0, 1),\n",
    "    'lambda_l2': uniform(0, 1)\n",
    "}\n",
    "\n",
    "# Crear el modelo base\n",
    "base_model = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    metric='rmse',\n",
    "    device='gpu',\n",
    "    gpu_platform_id=0,\n",
    "    gpu_device_id=1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Combinar datos de entrenamiento y validación\n",
    "X_combined = pd.concat([X_train, X_val])\n",
    "y_combined = pd.concat([y_train, y_val])\n",
    "weights_combined = pd.concat([weights_train, weights_val])\n",
    "\n",
    "# Crear un array de índices para PredefinedSplit\n",
    "test_fold = [-1] * len(X_train) + [0] * len(X_val)\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "# Configurar y ejecutar la búsqueda aleatoria\n",
    "random_search = RandomizedSearchCV(\n",
    "    base_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,\n",
    "    cv=ps,\n",
    "    random_state=42,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "# Definir fit_params para incluir los pesos y el conjunto de validación\n",
    "fit_params = {\n",
    "    \"sample_weight\": weights_combined,\n",
    "    \"eval_set\": [(X_val, y_val)],\n",
    "    \"eval_sample_weight\": [weights_val],\n",
    "    \"callbacks\": [lgb.early_stopping(stopping_rounds=50)]\n",
    "}\n",
    "\n",
    "# Ajustar el modelo\n",
    "random_search.fit(X_combined, y_combined, **fit_params)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Realizar predicciones y evaluar\n",
    "val_preds = best_model.predict(X_val)\n",
    "\n",
    "mse = mean_squared_error(y_val, val_preds)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val, val_preds)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "print(\"Mejores parámetros:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve-Labo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
